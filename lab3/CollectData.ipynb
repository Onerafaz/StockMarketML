{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup (Imports)\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "\n",
    "import yqd\n",
    "\n",
    "from Database import add_stock_ticks, add_headlines, clean_ticks, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def consume_ticker_csv(stock, filename):\n",
    "    \"\"\"Loads data from csv file into database\"\"\"\n",
    "    entries = []\n",
    "    \n",
    "    with open(os.path.join('..', 'data', filename), 'r') as tick_csv:\n",
    "        \n",
    "        for line in tick_csv:\n",
    "            \n",
    "            if \"Date\" not in line:\n",
    "                \n",
    "                date, open_, high, low, close, adj_close, volume = line.split(',')\n",
    "                \n",
    "                entries.append((stock, date, open_, high, low, close, adj_close, volume))\n",
    "                \n",
    "    add_stock_ticks(entries)\n",
    "    \n",
    "    clean_ticks()\n",
    "\n",
    "def dl_ticker(stock, num_days=10):\n",
    "    \"\"\"Loads data from yahoo\"\"\"\n",
    "    entries = []\n",
    "    \n",
    "    end_date = datetime.today()\n",
    "    begin_date = end_date - timedelta(days=num_days)\n",
    "    \n",
    "    for line in yqd.load_yahoo_quote(stock, begin_date.strftime('%Y%m%d'), end_date.strftime('%Y%m%d')):\n",
    "        \n",
    "        if \"Date\" not in line and len(line) > 1:\n",
    "                \n",
    "                date, open_, high, low, close, adj_close, volume = line.split(',')\n",
    "                \n",
    "                entries.append((stock, date, open_, high, low, close, adj_close, volume))\n",
    "                \n",
    "    add_stock_ticks(entries)\n",
    "    \n",
    "    clean_ticks()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def basic_clean(text):\n",
    "    return text.strip().replace(\"&#39;\", \"'\").replace(\"&quot;\", \"\").replace(\"&amp;\", \"and\").replace(\"(TM)\", \"\")\n",
    "\n",
    "def get_reddit_news(subs, search_terms, limit=None, praw_config='StockMarketML'):\n",
    "    \"Get headlines from Reddit\"\n",
    "    print('Downloading Reddit Posts: ' + \", \".join(subs))\n",
    "    \n",
    "    from praw import Reddit\n",
    "    \n",
    "    reddit = Reddit(praw_config)\n",
    "\n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    used = []\n",
    "    \n",
    "    for term in search_terms:\n",
    "\n",
    "        for submission in reddit.subreddit('+'.join(subs)).search(term, limit=limit):\n",
    "            \n",
    "            if submission.title.count(' ') > 4 and submission.title not in used:\n",
    "                \n",
    "                used.append(submission.title)\n",
    "                \n",
    "                date_key = datetime.fromtimestamp(submission.created).strftime('%Y-%m-%d')\n",
    "\n",
    "                articles[date_key].append(submission.title)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def get_reuters_news(stock, pages=80):\n",
    "    \"\"\"Get headlines from Reuters\"\"\"\n",
    "    print('Downloading Reuters: ' + stock)\n",
    "    \n",
    "    found_headlines = []\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    pattern_headline = re.compile('<h2><a [\\s\\S]+?>([\\s\\S]+?)<\\/a>[\\s\\S]*?<\\/h2>')\n",
    "    \n",
    "    date_current = datetime.now()\n",
    "    \n",
    "    while pages > 0:\n",
    "\n",
    "        text = requests.get('http://www.reuters.com/finance/stocks/company-news/{}?date={}'.format(stock, date_current.strftime('%m%d%Y')),  headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'}).text\n",
    "        \n",
    "        for match in pattern_headline.finditer(text):\n",
    "            \n",
    "            headline = match.group(1)\n",
    "            \n",
    "            headline = headline.replace('\\u200d', '').replace('\\u200b', '')\n",
    "            \n",
    "            headline = re.sub('^[A-Z]+[A-Z\\d\\s]*\\-', '', headline)\n",
    "            \n",
    "            date_key = date_current.strftime('%Y-%m-%d')\n",
    "            \n",
    "            if headline not in found_headlines:\n",
    "            \n",
    "                articles[date_key].append(headline)\n",
    "                found_headlines.append(headline)\n",
    "        \n",
    "        pages -= 1\n",
    "        \n",
    "        date_current -= timedelta(days=1)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def get_twitter_news(querys, limit=100):\n",
    "    \"\"\"Get headlines from Twitter\"\"\"\n",
    "    print('Downloading Tweets: ' + \", \".join(querys))\n",
    "    \n",
    "    from twitter import Twitter, OAuth\n",
    "    import twitter_creds as c # Self-Created Python file with Creds\n",
    "\n",
    "    twitter = Twitter(auth=OAuth(c.ACCESS_TOKEN, c.ACCESS_SECRET, c.CONSUMER_KEY, c.CONSUMER_SECRET))\n",
    "    \n",
    "    limit = min(limit, 100)\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    for query in querys:\n",
    "    \n",
    "        tweets = twitter.search.tweets(q=query, result_type='popular', lang='en', count=limit)['statuses']\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            \n",
    "            text = re.sub(r'https?:\\/\\/\\S+', '', tweet['text'])\n",
    "            text = re.sub(r'[^\\w\\s:/]+', '', text)\n",
    "            \n",
    "            date = tweet['created_at']\n",
    "            \n",
    "            if '\\n' not in text and len(text) > len(query) and ' ' in text:\n",
    "                \n",
    "                date_key = datetime.strptime(date, \"%a %b %d %H:%M:%S %z %Y\" ).strftime('%Y-%m-%d')\n",
    "                \n",
    "                articles[date_key].append(text)\n",
    "                \n",
    "    return articles\n",
    "\n",
    "def get_seekingalpha_news(stock, pages=500):\n",
    "    \"\"\"Get headlines from SeekingAlpha\"\"\"\n",
    "    print('Downloading SeekingAlpha: ' + stock)\n",
    "\n",
    "    articles = defaultdict(list)\n",
    "\n",
    "    re_headline = re.compile('<a class=\"market_current_title\" [\\s\\S]+?>([\\s\\S]+?)<\\/a>')\n",
    "    re_dates = re.compile('<span class=\"date pad_on_summaries\">([\\s\\S]+?)<\\/span>')\n",
    "\n",
    "    cookies = None\n",
    "\n",
    "    for i in range(1, pages + 1):\n",
    "\n",
    "        if i == 1:\n",
    "            url = 'https://seekingalpha.com/symbol/{}/news'.format(stock)\n",
    "        else:\n",
    "            url = 'https://seekingalpha.com/symbol/{}/news/more_news_all?page={}'.format(stock, i)\n",
    "            \n",
    "        try:\n",
    "\n",
    "            r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'}, cookies=cookies)\n",
    "        \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(e)\n",
    "            continue\n",
    "    \n",
    "        text = r.text.replace('\\\\\"', '\"')\n",
    "        cookies = r.cookies # SeekingAlpha wants cookies.\n",
    "\n",
    "        headlines = [match.group(1) for match in re_headline.finditer(text)]\n",
    "        dates = [match.group(1) for match in re_dates.finditer(text)]\n",
    "\n",
    "        for headline, date in zip(headlines, dates):\n",
    "            \n",
    "            headline = headline.replace('(update)', '')\n",
    "            \n",
    "            date = date.replace('.', '')\n",
    "\n",
    "            if 'Today' in date:\n",
    "                date = datetime.today()\n",
    "            elif 'Yesterday' in date:\n",
    "                date = datetime.today() - timedelta(days=1)\n",
    "            else:\n",
    "                temp = date.split(',')\n",
    "                if len(temp[0]) == 3:\n",
    "                    date = datetime.strptime(temp[1], \" %b %d\").replace(year=datetime.today().year)\n",
    "                else:\n",
    "                    date = datetime.strptime(\"\".join(temp[0:2]), \"%b %d %Y\")\n",
    "\n",
    "            articles[date.strftime('%Y-%m-%d')].append(headline)\n",
    "\n",
    "    return articles\n",
    "\n",
    "def get_fool_news(stock, pages=40):\n",
    "    \"Get headlines from Motley Fool\"\n",
    "    print('Downloading MotleyFool: ' + stock)\n",
    "    \n",
    "    stock = stock.lower()\n",
    "    \n",
    "    re_headline = re.compile('<article id=\"article-\\d+\">[\\s\\S]+?\">([\\s\\S]+?)<\\/a>[\\s\\S]+?calendar\"><\\/i>([\\s\\S]+?20\\d{2})')\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    for i in range(pages):\n",
    "        \n",
    "        if i == 0:\n",
    "            url = \"https://www.fool.com/quote/nasdaq/apple/{}/content\".format(stock)\n",
    "        else:\n",
    "            url = \"https://www.fool.com/quote/nasdaq/apple/{}/content/more?page={}\".format(stock, i)\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            text = requests.get(url).text\n",
    "            \n",
    "        except: # Timeout or something...\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        headlines = [(match.group(1), match.group(2)) for match in re_headline.finditer(text)]\n",
    "        \n",
    "        for headline, date in headlines:\n",
    "            \n",
    "            date = datetime.strptime(date.strip(), \"%b %d %Y\")\n",
    "            headline = basic_clean(headline)\n",
    "            \n",
    "            articles[date.strftime('%Y-%m-%d')].append(headline)\n",
    "            \n",
    "    return articles\n",
    "\n",
    "def get_wsj(stock, pages=20):\n",
    "    \"Get headlines from WSJ\"\n",
    "    print('Downloading WSJ: ' + stock)\n",
    "    \n",
    "    re_headline = re.compile('<li class=\"\\s+cr_pressRelease\">[\\s\\S]+?\"cr_dateStamp\">([\\s\\S]+?)<\\/li>[\\s\\S]+?href=\"http:\\/\\/www.wsj.com\\/articles\\S+?>([\\s\\S]+?)<\\/a>')\n",
    "    re_nextlink = re.compile('article_datetime\" value=\"?(\\d+\\/\\d+\\/\\d+)\"?>[\\s\\S]+?article_docId\" value=\"?(\\d+)\"?>[\\s\\S]+?newswire_datetime\" value=\"?(\\d+\\/\\d+\\/\\d+)\"?>[\\s\\S]+?newswire_docId\" value=\"?(\\d+)\"?>[\\s\\S]+?')\n",
    "    \n",
    "    url = \"http://quotes.wsj.com/ajax/overview/5/US/{}?instrumentType=STOCK&significant=false\".format(stock)\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    for i in range(pages):\n",
    "        \n",
    "        text = requests.get(url).text\n",
    "        \n",
    "        headlines = [(match.group(1), match.group(2)) for match in re_headline.finditer(text)]\n",
    "        \n",
    "        for date, headline in headlines:\n",
    "            \n",
    "            try:\n",
    "                date = datetime.strptime(date.strip(), \"%m/%d/%y\")\n",
    "            except:\n",
    "                date = datetime.today()\n",
    "                \n",
    "            headline = basic_clean(headline)\n",
    "            \n",
    "            articles[date.strftime('%Y-%m-%d')].append(headline)\n",
    "            \n",
    "        nextpage_creds = re_nextlink.search(text)\n",
    "        \n",
    "        if nextpage_creds:\n",
    "            \n",
    "            nextpage_creds = [nextpage_creds.group(1), nextpage_creds.group(2), nextpage_creds.group(3), nextpage_creds.group(4)]\n",
    "            url = \"http://quotes.wsj.com/ajax/overview/5/US/{}?instrumentType=STOCK&significant=false&article_datetime={}&article_docId={}&newswire_datetime={}&newswire_docId={}\".format(stock, *nextpage_creds)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            break\n",
    "            \n",
    "    return articles\n",
    "\n",
    "def get_thestreet(stock, pages=60):\n",
    "    \"Get headlines from TheStreet\"\n",
    "    print('Downloading TheStreet: ' + stock)\n",
    "    \n",
    "    url = \"https://www.thestreet.com/quote/{}/details/news?start=0&type=json\".format(stock)\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    for i in range(pages):\n",
    "    \n",
    "        try:\n",
    "            json = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'}).json()\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "        for story in json['stories']:\n",
    "            \n",
    "            if 'headline' and 'callout' in story and story['headline'] and story['callout']:\n",
    "            \n",
    "                headline = basic_clean(story['headline'])\n",
    "                callout = basic_clean(story['callout'])\n",
    "                date = datetime.strptime(story['publishDate'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "                articles[date.strftime('%Y-%m-%d')].append(headline)\n",
    "                articles[date.strftime('%Y-%m-%d')].append(callout)\n",
    "            \n",
    "        url = \"https://www.thestreet.com\" + json['pagination']['nextDataUrl']\n",
    "        \n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_headline(headline, dictionary):\n",
    "    \"\"\"\n",
    "    Clean headline\n",
    "    \n",
    "    Removes extra chars and replaces words\n",
    "    \"\"\"\n",
    "    headline = headline.lower()\n",
    "    headline = re.sub('\\d+%', 'STAT', headline)\n",
    "    headline = re.sub('\\b\\d+\\b', 'STAT', headline)\n",
    "    headline = ''.join(c for c in headline if c in \"abcdefghijklmnopqrstuvwxyz \")\n",
    "    headline = re.sub('\\s+', ' ', headline)\n",
    "        \n",
    "    for (word, replacement) in dictionary:\n",
    "            \n",
    "        headline = headline.replace(word, replacement)\n",
    "        \n",
    "    headline = headline.replace('STAT', '**STATISTIC**')\n",
    "        \n",
    "    headline = headline.replace('****', '** **') # Seperate joined kwords\n",
    "    \n",
    "    return headline.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_headlines(headlines):\n",
    "    \"\"\"Save headlines to file\"\"\"\n",
    "    \n",
    "    for stock in headlines:\n",
    "        \n",
    "        entries = []\n",
    "        \n",
    "        with db() as (conn, cur):\n",
    "        \n",
    "            cur.execute(\"SELECT word, replacement FROM dictionary WHERE stock=? ORDER BY LENGTH(word) DESC\", [stock])\n",
    "            dictionary = cur.fetchall()\n",
    "        \n",
    "        for source in headlines[stock]:\n",
    "            \n",
    "            for date in headlines[stock][source]:\n",
    "                \n",
    "                for headline in headlines[stock][source][date]:\n",
    "                    \n",
    "                    cleaned_headline = clean_headline(headline, dictionary)\n",
    "                    \n",
    "                    entries.append((stock, date, source, cleaned_headline, headline, -999))\n",
    "                    \n",
    "        add_headlines(entries)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Reuters: GOOG.O\n",
      "Downloading SeekingAlpha: GOOG\n",
      "Downloading MotleyFool: GOOG\n",
      "Downloading WSJ: GOOG\n",
      "Downloading TheStreet: GOOG\n",
      "Downloading Reuters: AAPL.O\n",
      "Downloading SeekingAlpha: AAPL\n",
      "Downloading MotleyFool: AAPL\n",
      "Downloading WSJ: AAPL\n",
      "Downloading TheStreet: AAPL\n",
      "Downloading Reuters: MSFT.O\n",
      "Downloading SeekingAlpha: MSFT\n",
      "Downloading MotleyFool: MSFT\n",
      "Downloading WSJ: MSFT\n",
      "Downloading TheStreet: MSFT\n",
      "Downloading Reuters: AMD.O\n",
      "Downloading SeekingAlpha: AMD\n",
      "Downloading MotleyFool: AMD\n",
      "Downloading WSJ: AMD\n",
      "Downloading TheStreet: AMD\n",
      "Downloading Reuters: AMZN.O\n",
      "Downloading SeekingAlpha: AMZN\n",
      "Downloading MotleyFool: AMZN\n",
      "Downloading WSJ: AMZN\n",
      "Downloading TheStreet: AMZN\n",
      "Downloading Reuters: INTC.O\n",
      "Downloading SeekingAlpha: INTC\n",
      "Downloading MotleyFool: INTC\n",
      "Downloading WSJ: INTC\n",
      "Downloading TheStreet: INTC\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    headlines = {\n",
    "            'GOOG': {\n",
    "                # 'reddit': get_reddit_news(['google', 'Android', 'GooglePixel', 'news'], ['Google', 'pixel', 'android', 'stock']), \n",
    "                'reuters': get_reuters_news('GOOG.O'),\n",
    "                # 'twitter': get_twitter_news(['@Google', '#Google', '#googlepixel', '#Alphabet']),\n",
    "                'seekingalpha': get_seekingalpha_news('GOOG'),\n",
    "                'fool': get_fool_news('GOOG'),\n",
    "                'wsj': get_wsj('GOOG'),\n",
    "                'thestreet': get_thestreet('GOOG')\n",
    "            },\n",
    "            'AAPL': {\n",
    "                # 'reddit': get_reddit_news(['apple', 'ios', 'AAPL', 'news'], ['apple', 'iphone', 'ipad', 'ios', 'stock']), \n",
    "                'reuters': get_reuters_news('AAPL.O'),\n",
    "                # 'twitter': get_twitter_news(['@Apple', '#Apple', '#IPhone', '#ios']),\n",
    "                'seekingalpha': get_seekingalpha_news('AAPL'),\n",
    "                'fool': get_fool_news('AAPL'),\n",
    "                'wsj': get_wsj('AAPL'),\n",
    "                'thestreet': get_thestreet('AAPL')\n",
    "            },\n",
    "            'MSFT': {\n",
    "                # 'reddit': get_reddit_news(['microsoft', 'windowsphone', 'windows'], ['microsoft', 'phone', 'windows', 'stock']), \n",
    "                'reuters': get_reuters_news('MSFT.O'),\n",
    "                # 'twitter': get_twitter_news(['@Microsoft', '#Windows', '#Microsoft', '#windowsphone']),\n",
    "                'seekingalpha': get_seekingalpha_news('MSFT'),\n",
    "                'fool': get_fool_news('MSFT'),\n",
    "                'wsj': get_wsj('MSFT'),\n",
    "                'thestreet': get_thestreet('MSFT')\n",
    "            },\n",
    "            'AMD': {\n",
    "                # 'reddit': get_reddit_news(['Amd', 'AMD_Stock', 'pcmasterrace'], ['AMD', 'radeon', 'ryzen', 'stock']), \n",
    "                'reuters': get_reuters_news('AMD.O'),\n",
    "                # 'twitter': get_twitter_news(['@AMD', '#AMD', '#Ryzen', '#radeon']),\n",
    "                'seekingalpha': get_seekingalpha_news('AMD'),\n",
    "                'fool': get_fool_news('AMD'),\n",
    "                'wsj': get_wsj('AMD'),\n",
    "                'thestreet': get_thestreet('AMD')\n",
    "            },\n",
    "            'AMZN': {\n",
    "                # 'reddit': get_reddit_news(['amazon', 'amazonprime', 'amazonecho'], ['amazon', 'echo', 'prime', 'stock']), \n",
    "                'reuters': get_reuters_news('AMZN.O'),\n",
    "                # 'twitter': get_twitter_news(['@amazon', '#Amazon', '#jeffbezos', '@amazonecho', '#amazonprime']),\n",
    "                'seekingalpha': get_seekingalpha_news('AMZN'),\n",
    "                'fool': get_fool_news('AMZN'),\n",
    "                'wsj': get_wsj('AMZN'),\n",
    "                'thestreet': get_thestreet('AMZN')\n",
    "            },\n",
    "            'INTC': {\n",
    "                # 'reddit': get_reddit_news(['intel', 'hardware'], ['intel', 'cpu']),\n",
    "                'reuters': get_reuters_news('INTC.O'),\n",
    "                # 'twitter': get_twitter_news(['@intel']),\n",
    "                'seekingalpha': get_seekingalpha_news('INTC'),\n",
    "                'fool': get_fool_news('INTC'),\n",
    "                'wsj': get_wsj('INTC'),\n",
    "                'thestreet': get_thestreet('INTC')\n",
    "            }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    save_headlines(headlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('headlines_test.json', 'w') as fp:\n",
    "#     json.dump(headlines, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1347\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1298\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m             self.sock = self._context.wrap_socket(self.sock,\n\u001b[0m\u001b[1;32m   1423\u001b[0m                                                   server_hostname=server_hostname)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1039\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6z/171c1kh54g7_fb5_12ckxchw0000gn/T/ipykernel_13201/383907557.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdl_ticker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AAPL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdl_ticker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AMZN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdl_ticker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AMD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6z/171c1kh54g7_fb5_12ckxchw0000gn/T/ipykernel_13201/3216773020.py\u001b[0m in \u001b[0;36mdl_ticker\u001b[0;34m(stock, num_days)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbegin_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_date\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_days\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myqd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_yahoo_quote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"Date\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/StockMarketML/lab3/yqd.py\u001b[0m in \u001b[0;36mload_yahoo_quote\u001b[0;34m(ticker, begindate, enddate, info)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_cookie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_crumb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cookie\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_crumb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0m_get_cookie_crumb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# Prepare the parameters and the URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/StockMarketML/lab3/yqd.py\u001b[0m in \u001b[0;36m_get_cookie_crumb\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Perform a Yahoo financial lookup on SP500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://finance.yahoo.com/quote/^GSPC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0malines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    535\u001b[0m                                   '_open', req)\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1390\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)>"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    dl_ticker('AAPL')\n",
    "    dl_ticker('AMZN')\n",
    "    dl_ticker('AMD')\n",
    "    dl_ticker('GOOG')\n",
    "    dl_ticker('MSFT')\n",
    "    dl_ticker('INTC')\n",
    "    dl_ticker('CL=F')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
