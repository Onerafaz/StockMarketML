{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from Database import db\n",
    " \n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from keras.optimizers import RMSprop, Adam, Nadam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Nadam\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, concatenate, SpatialDropout1D, GRU\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM, Activation, BatchNormalization, Dropout, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import keras.backend as K\n",
    "# from keras.utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "\n",
    "stocks      = ['AMD', 'INTC', 'AAPL', 'AMZN', 'MSFT', 'GOOG']\n",
    "all_sources = ['reuters', 'seekingalpha', 'fool', 'wsj', 'thestreet']\n",
    "\n",
    "model_type  = 'multiheadlineclf'\n",
    "\n",
    "doc2vec_options = dict(\n",
    "    size=300, \n",
    "    window=10, \n",
    "    min_count=5,\n",
    "    workers=10,\n",
    "    alpha=0.025, \n",
    "    min_alpha=0.025, \n",
    "    max_vocab_size=15000,\n",
    "    dm=1\n",
    ")\n",
    "\n",
    "keras_options = dict(\n",
    "    epochs=200, \n",
    "    batch_size=64,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "tick_window = 18\n",
    "doc_query_days = 6\n",
    "combined_emb_size = 5 + doc2vec_options['size']\n",
    "\n",
    "test_cutoff = datetime(2018, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_time(date, days):\n",
    "    \n",
    "    return (date + timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "\n",
    "def clean(sentence):\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('-', ' ').replace('_', ' ').replace('&', ' ')\n",
    "    sentence = ''.join(char for char in sentence if char in \"abcdefghijklmnopqrstuvwxyz.!? \")\n",
    "    sentence = re.sub('\\s+', ' ', sentence).strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def clean2(sentence): # Clean already cleaned headline (aka return this)\n",
    "    return sentence\n",
    "\n",
    "def make_doc_embeddings(query_range=(None, '1776-07-04', '3000-01-01'), use_extra_dates=True, vec_model=None):\n",
    "    \"\"\"\n",
    "    Create document embeddings from headlines\n",
    "    \"\"\"\n",
    "    if not vec_model: print('Creating doc embeddings...')\n",
    "\n",
    "    docs, labels = [], []\n",
    "    \n",
    "    class LabeledLineSentence:\n",
    "        \n",
    "        def __init__(self, docs, labels):\n",
    "            self.docs = docs\n",
    "            self.labels = labels\n",
    "            \n",
    "        def __iter__(self):\n",
    "            for idx, doc in enumerate(self.docs):\n",
    "                yield TaggedDocument(doc.split(), [self.labels[idx]]) # clean doc\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        q_stock, q_start, q_end = query_range\n",
    "        \n",
    "        for stock in stocks:\n",
    "            \n",
    "            ## Headline For Every Date ##\n",
    "            \n",
    "            if q_stock and q_stock != stock:\n",
    "                continue\n",
    "            \n",
    "            cur.execute(\"SELECT DISTINCT date FROM headlines WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC\", [stock, q_start, q_end])\n",
    "            dates = [date[0] for date in cur.fetchall()]\n",
    "            \n",
    "            if use_extra_dates: # True headline days not enough so we create additional querys\n",
    "                new_dates = []\n",
    "                for date in dates: \n",
    "                    d = datetime.strptime(date, '%Y-%m-%d')\n",
    "                    new_dates.append(add_time(d, -1))\n",
    "                    new_dates.append(add_time(d, +1))\n",
    "                dates.extend(new_dates)\n",
    "                \n",
    "            if not vec_model: # Show loading bar only for training data\n",
    "                date_iter = tqdm_notebook(dates, desc=stock)\n",
    "            else:\n",
    "                date_iter = iter(dates)\n",
    "            \n",
    "            for date in date_iter:\n",
    "                \n",
    "                ## Collect Headlines ##\n",
    "                \n",
    "                event_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "                \n",
    "                cur.execute(\"SELECT date, source, content FROM headlines WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC\", \n",
    "                            [stock, add_time(event_date, -doc_query_days), date])\n",
    "                headlines = [(date, source, clean2(content), (event_date - datetime.strptime(date, '%Y-%m-%d')).days) \n",
    "                                 for (date, source, content) in cur.fetchall() if content]\n",
    "                \n",
    "                if len(headlines) == 0:\n",
    "                    continue\n",
    "                \n",
    "                ## Create training example ##\n",
    "                    \n",
    "                contents = [headline[2] for headline in headlines]\n",
    "\n",
    "                doc = \" **NEXT** \".join(contents)\n",
    "                \n",
    "                docs.append(doc)\n",
    "                labels.append(stock + \" \" + date)\n",
    "                \n",
    "    vectors = {stock: {} for stock in stocks}\n",
    "            \n",
    "    doc_iter = LabeledLineSentence(docs, labels)\n",
    "    \n",
    "    if not vec_model:\n",
    "        \n",
    "        vec_model = Doc2Vec(documents=doc_iter, **doc2vec_options)\n",
    "        #     vec_model = Doc2Vec(**doc2vec_options)\n",
    "        #     vec_model.build_vocab(doc_iter)\n",
    "\n",
    "        #     for epoch in range(100):\n",
    "        #         vec_model.train(doc_iter, **doc2vec_options)\n",
    "        #         vec_model.alpha -= 0.002\n",
    "        #         vec_model.min_alpha = vec_model.alpha\n",
    "        \n",
    "        for label in labels:\n",
    "        \n",
    "            stock, date = label.split(\" \")\n",
    "\n",
    "            vectors[stock][date] = vec_model.docvecs[label]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        for tag_doc in doc_iter:\n",
    "            \n",
    "            vec = vec_model.infer_vector(tag_doc.words, \n",
    "                                         alpha=doc2vec_options['alpha'], \n",
    "                                         min_alpha=doc2vec_options['min_alpha'], \n",
    "                                         steps=1000)\n",
    "            \n",
    "            stock, date = tag_doc.tags[0].split(\" \")\n",
    "            \n",
    "            vectors[stock][date] = vec\n",
    "            \n",
    "    return vec_model, vectors, (docs, labels)\n",
    "\n",
    "def make_tick_data(query_range=(None, '1776-07-04', '3000-01-01'), train=True):\n",
    "    \"\"\"\n",
    "    Process historic tick data (high/low/close/etc..) into training examples\n",
    "    \"\"\"\n",
    "    if train: print('Creating tick data...')\n",
    "    \n",
    "    tick_vecs = {stock: {} for stock in stocks}\n",
    "    effect_vecs = {stock: {} for stock in stocks}\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        q_stock, q_start, q_end = query_range\n",
    "        \n",
    "        for stock in stocks:\n",
    "            \n",
    "            if q_stock and q_stock != stock:\n",
    "                continue\n",
    "            \n",
    "            cur.execute(\"SELECT DISTINCT date FROM headlines WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC LIMIT 1\", [stock, q_start, q_end])\n",
    "            start_date = cur.fetchall()[0][0]\n",
    "            \n",
    "            cur.execute(\"SELECT DISTINCT date FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC\", [stock, start_date, q_end])\n",
    "            dates = [date[0] for date in cur.fetchall()]\n",
    "            \n",
    "            for date in dates:\n",
    "                \n",
    "                event_date = datetime.strptime(date, '%Y-%m-%d') # The date of headline\n",
    "\n",
    "                ## Find corresponding tick data ## \n",
    "\n",
    "                cur.execute(\"\"\"SELECT open, high, low, adjclose, volume FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date DESC LIMIT 52\"\"\", \n",
    "                            [stock, \n",
    "                             add_time(event_date, -80), \n",
    "                             add_time(event_date, 0)])\n",
    "\n",
    "                before_headline_ticks = cur.fetchall()\n",
    "\n",
    "                if len(before_headline_ticks) < tick_window:\n",
    "                    continue\n",
    "                    \n",
    "                if train:\n",
    "\n",
    "                    cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC LIMIT 1\"\"\", \n",
    "                                [stock, \n",
    "                                add_time(event_date, 1), \n",
    "                                add_time(event_date, 4)])\n",
    "\n",
    "                    after_headline_ticks = cur.fetchall()\n",
    "\n",
    "                    if len(after_headline_ticks) == 0 and train:\n",
    "                        continue\n",
    "                    \n",
    "                ## Create ##\n",
    "\n",
    "                window_ticks = np.array(list(reversed(before_headline_ticks[:tick_window]))) # Flip so in chron. order\n",
    "                fifty_ticks = np.array(before_headline_ticks) # Use last 50 ticks to normalize\n",
    "\n",
    "                previous_tick = before_headline_ticks[0][3]\n",
    "                \n",
    "                if train:\n",
    "                    result_tick = after_headline_ticks[0][0]\n",
    "\n",
    "                if previous_tick:\n",
    "\n",
    "                    window_ticks -= np.mean(fifty_ticks, axis=0)\n",
    "                    window_ticks /= np.std(fifty_ticks, axis=0)\n",
    "                    \n",
    "                    tick_vecs[stock][date] = window_ticks\n",
    "                    \n",
    "                    if train:\n",
    "                    \n",
    "                        if result_tick > previous_tick:\n",
    "                            effect = [1., 0.]\n",
    "                        else:\n",
    "                            effect = [0., 1.]\n",
    "\n",
    "                        effect_vecs[stock][date] = effect\n",
    "                    \n",
    "    return tick_vecs, effect_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def merge_data(doc_vecs, tick_vecs, effect_vecs=None):\n",
    "    \"\"\"\n",
    "    Pairs document and tick vectors (both timeseries) to an effect vector (up/down)\n",
    "    \"\"\"\n",
    "    if effect_vecs: print('Creating X, Y...')\n",
    "    \n",
    "    X, Y, test_indices = [], [], []\n",
    "    \n",
    "    for stock in stocks:\n",
    "        \n",
    "        for date, tick_vec in tick_vecs[stock].items():\n",
    "            \n",
    "            x = []\n",
    "            \n",
    "            if effect_vecs:\n",
    "                y = effect_vecs[stock][date]\n",
    "            \n",
    "            event_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "            \n",
    "            window_dates = [add_time(event_date, -i) for i in range(tick_window)]\n",
    "            \n",
    "            for i in range(tick_window):\n",
    "                \n",
    "                if window_dates[i] not in doc_vecs[stock]:\n",
    "                    break\n",
    "                    \n",
    "                x_i = np.concatenate([tick_vec[i], doc_vecs[stock][window_dates[i]]]) # Combine tick data and doc data\n",
    "                \n",
    "                x.append(x_i)\n",
    "                \n",
    "            if len(x) == tick_window:\n",
    "                \n",
    "                X.append(x)\n",
    "                \n",
    "                if effect_vecs:\n",
    "                    \n",
    "                    Y.append(y)\n",
    "                \n",
    "                    if event_date > test_cutoff: # Label as test data\n",
    "                        test_indices.append(len(X) - 1)\n",
    "        \n",
    "    return np.array(X), np.array(Y), np.array(test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X, Y, test_indices):\n",
    "    \"\"\"\n",
    "    Splits X/Y to Train/Test\n",
    "    \"\"\"\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = np.setdiff1d(indices, test_indices, assume_unique=True)\n",
    "    \n",
    "    trainX,  testX  = X[train_indices],  X[test_indices]\n",
    "    trainY,  testY  = Y[train_indices],  Y[test_indices]\n",
    "    \n",
    "    return trainX, trainY, testX, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def correct_sign_acc(y_true, y_pred): # Currently not used\n",
    "    \"\"\"\n",
    "    Accuracy of Being Positive or Negative\n",
    "    \"\"\"\n",
    "    diff = K.equal(y_true > 0, y_pred > 0)\n",
    "    \n",
    "    return K.mean(diff, axis=-1)\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    model_input = Input(shape=(tick_window, combined_emb_size), name=\"Input\")\n",
    "    \n",
    "    rnn = LSTM(400, return_sequences=False)(model_input)\n",
    "    rnn = Dropout(0.2)(rnn)\n",
    "    \n",
    "    dense = Dense(400)(rnn)\n",
    "    dense = Activation('selu')(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    dense = Dense(400)(rnn)\n",
    "    dense = Activation('selu')(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    dense = Dense(2)(dense)\n",
    "    pred_output = Activation('softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=model_input, outputs=pred_output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=0.008), loss='mse', metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating doc embeddings...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6z/171c1kh54g7_fb5_12ckxchw0000gn/T/ipykernel_13921/2060871784.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mvec_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_doc_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#vec_model.docvecs.most_similar(\"INTC 2016-04-20\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtick_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffect_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tick_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6z/171c1kh54g7_fb5_12ckxchw0000gn/T/ipykernel_13921/1682612733.py\u001b[0m in \u001b[0;36mmake_doc_embeddings\u001b[0;34m(query_range, use_extra_dates, vec_model)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvec_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Show loading bar only for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdate_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mdate_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/__init__.py\u001b[0m in \u001b[0;36mtqdm_notebook\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m          \u001b[0;34m\"Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m          TqdmDeprecationWarning, stacklevel=2)\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Prepare IPython progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    vec_model, doc_vecs, doc_data = make_doc_embeddings() #vec_model.docvecs.most_similar(\"INTC 2016-04-20\")\n",
    "    \n",
    "    tick_vecs, effect_vecs = make_tick_data()\n",
    "    \n",
    "    X, Y, test_indices = merge_data(doc_vecs, tick_vecs, effect_vecs)\n",
    "    \n",
    "    trainX, trainY, testX, testY = split_data(X, Y, test_indices)\n",
    "    \n",
    "    print(trainX.shape, testY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    " \n",
    "    ## Create Model ##\n",
    "    \n",
    "    model = get_model()\n",
    "    \n",
    "    monitor_mode = 'acc'\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(datetime.now().strftime(\"%Y,%m,%d-%H,%M,%S,tick,\" + model_type)))\n",
    "    e_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    checkpoint = ModelCheckpoint(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'), \n",
    "                                 monitor=monitor_mode,\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True)\n",
    "    \n",
    "    vec_model.save(os.path.join('..', 'models', 'doc2vec-' + model_type + '.doc2vec'))\n",
    "    \n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "    ## Train ##\n",
    "    \n",
    "    history = model.fit(trainX,\n",
    "                        trainY,\n",
    "                        validation_data=(testX, testY),\n",
    "                        callbacks=[e_stopping, tensorboard, checkpoint],\n",
    "                        **keras_options)\n",
    "    \n",
    "    ## Display Train History ##\n",
    "    \n",
    "    plt.plot(np.log(history.history['loss']))\n",
    "    plt.plot(np.log(history.history['val_loss']))\n",
    "    plt.legend(['LogTrainLoss', 'LogTestLoss'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history[monitor_mode])\n",
    "    plt.plot(history.history['val_' + monitor_mode])\n",
    "    plt.legend(['TrainAcc', 'TestAcc'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TEST] AoC\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        actualY = testY\n",
    "        predictY = model.predict(testX)\n",
    "        \n",
    "        print(\"ROC\", roc_auc_score(actualY, predictY))\n",
    "        \n",
    "        print(confusion_matrix(testY[:, 0] > .5, predictY[:, 0] > .5))\n",
    "        \n",
    "    except NameError:\n",
    "        \n",
    "        print(\"Test Data and Model Required!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [TEST] Predict\n",
    "\n",
    "def predict(stock, model=None, vec_model=None, current_date=None, predict_date=None):\n",
    "    \n",
    "    ## Check Args ##\n",
    "    \n",
    "    if not model or not vec_model:\n",
    "        \n",
    "        vec_model = Doc2Vec.load(os.path.join('..', 'models', 'doc2vec-' + model_type + '.doc2vec'))\n",
    "    \n",
    "        model = load_model(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'))\n",
    "        \n",
    "    if not current_date:\n",
    "        current_date = datetime.today()\n",
    "        \n",
    "    if not predict_date:\n",
    "        predict_date = current_date + timedelta(days=1)\n",
    "        \n",
    "    ## Predict ##\n",
    "    \n",
    "    query_range = stock, add_time(current_date, -tick_window-1), add_time(current_date, 0)\n",
    "    \n",
    "    vec_model, doc_vecs, _ = make_doc_embeddings(query_range=query_range, vec_model=vec_model)\n",
    "    \n",
    "    tick_vecs, _ = make_tick_data(query_range=query_range, train=False)\n",
    "    \n",
    "    X, _, _ = merge_data(doc_vecs, tick_vecs, None)\n",
    "    \n",
    "    pred = model.predict(X)\n",
    "        \n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TEST] Spot Testing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## **This Test May Overlap w/Train Data** ##\n",
    "    \n",
    "    ## Options ##\n",
    "    \n",
    "    stock = 'AMD'\n",
    "    current_date = '2018-05-07'\n",
    "    predict_date = '2018-05-08'\n",
    "    \n",
    "    ## Run ##\n",
    "    \n",
    "    pred = predict(stock, \n",
    "                   current_date=datetime.strptime(current_date, '%Y-%m-%d'), \n",
    "                   predict_date=datetime.strptime(predict_date, '%Y-%m-%d'))\n",
    "    \n",
    "    ## Find Actual Value ##\n",
    "     \n",
    "    with db() as (conn, cur):\n",
    "    \n",
    "        cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC LIMIT 1\"\"\", \n",
    "                        [stock, \n",
    "                        add_time(datetime.strptime(predict_date, '%Y-%m-%d'), 0), \n",
    "                        add_time(datetime.strptime(predict_date, '%Y-%m-%d'), 6)])\n",
    "\n",
    "        after_headline_ticks = cur.fetchall()\n",
    "        try:\n",
    "            actual_result = after_headline_ticks[0][0]\n",
    "        except:\n",
    "            actual_result = -1\n",
    "            \n",
    "    ## Display ##\n",
    "            \n",
    "    parse = lambda num: str(round(num, 2))\n",
    "    \n",
    "    print(np.argmin(pred, axis=1))\n",
    "    print(parse(np.mean(pred[:, 0])))\n",
    "    \n",
    "    print(\"Actual Price: \" + parse(actual_result))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
